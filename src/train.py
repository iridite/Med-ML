import albumentations as A
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from albumentations.pytorch import ToTensorV2
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±å†™çš„æ¨¡å—
from src.config import Config
from src.dataset import MelanomaDataset, get_preprocessed_df
from src.models import CausalFusionModel
from src.utils import seed_everything


def train_one_epoch(model, loader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    # è¿›åº¦æ¡
    pbar = tqdm(loader, desc=f"Epoch {epoch + 1} [Train]")

    for images, metas, targets, weights in pbar:
        # æ•°æ®ç§»åˆ° GPU
        images, metas = images.to(device), metas.to(device)
        targets = targets.to(device).unsqueeze(1)  # [batch] -> [batch, 1]
        weights = weights.to(device).unsqueeze(1)  # [batch] -> [batch, 1]

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        logits = model(images, metas)

        # === è®¡ç®—æŸå¤± (æ”¯æŒå› æœåŠ æƒ) ===
        # criterion è®¾å®šä¸º noneï¼Œè¿”å›æ¯ä¸ªæ ·æœ¬çš„loss
        raw_loss = criterion(logits, targets)

        # æ‰‹åŠ¨ä¹˜ä»¥æƒé‡ (å¦‚æœ weights å…¨æ˜¯ 1ï¼Œè¿™é‡Œå°±ç­‰äºæ²¡å˜)
        loss = (raw_loss * weights).mean()

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        pbar.set_postfix({"loss": running_loss / (pbar.n + 1)})

    return running_loss / len(loader)


def validate(model, loader, criterion, device):
    model.eval()
    val_loss = 0.0
    all_targets = []
    all_preds = []

    with torch.no_grad():
        for images, metas, targets, _ in tqdm(loader, desc="[Valid]"):
            images, metas = images.to(device), metas.to(device)
            targets_gpu = targets.to(device).unsqueeze(1)

            logits = model(images, metas)

            # Loss
            loss = criterion(logits, targets_gpu)
            val_loss += loss.item()

            # è®°å½•ç»“æœç”¨äºè®¡ç®— AUC
            all_targets.extend(targets.numpy())
            all_preds.extend(torch.sigmoid(logits).cpu().numpy())  # Logitsè½¬æ¦‚ç‡

    # è®¡ç®— AUC (Area Under Curve)
    try:
        auc = roc_auc_score(all_targets, all_preds)
    except ValueError:
        auc = 0.5  # é˜²æ­¢åªæœ‰ä¸€ä¸ªç±»åˆ«æ—¶æŠ¥é”™

    return val_loss / len(loader), auc


def main():
    seed_everything(Config.SEED)
    print(f"ä½¿ç”¨çš„è®¾å¤‡: {Config.DEVICE}")

    # 1. æ•°æ®å‡†å¤‡
    df = get_preprocessed_df(Config.TRAIN_CSV)

    # ç®€å•çš„å›¾åƒå¢å¼º (Resize -> Normalize -> Tensor)
    transforms_train = A.Compose(
        [
            A.Resize(Config.IMG_SIZE, Config.IMG_SIZE),
            A.HorizontalFlip(p=0.5),  # éšæœºæ°´å¹³ç¿»è½¬
            A.VerticalFlip(p=0.5),  # éšæœºå‚ç›´ç¿»è½¬
            A.Normalize(),
            ToTensorV2(),
        ]
    )
    transforms_val = A.Compose(
        [A.Resize(Config.IMG_SIZE, Config.IMG_SIZE), A.Normalize(), ToTensorV2()]
    )

    full_dataset = MelanomaDataset(df, Config.TRAIN_IMG_DIR, transform=transforms_train)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% / 20%)
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_ds, val_ds = random_split(full_dataset, [train_size, val_size])

    # è¿™é‡Œçš„ val_ds transform å…¶å®è¿˜æ²¡è¦†ç›–ï¼Œä¸ºäº†MVPä»£ç ç®€æ´å…ˆå…±ç”¨
    # (æ›´ä¸¥è°¨çš„åšæ³•æ˜¯é‡å†™ Dataset wrapper è¦†ç›– transformï¼Œå…ˆè·³è¿‡)

    train_loader = DataLoader(
        train_ds,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )
    val_loader = DataLoader(
        val_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4
    )

    # 2. æ¨¡å‹åˆå§‹åŒ–
    # æ³¨æ„ï¼šè¡¨æ ¼ç»´åº¦ä» full_dataset è‡ªåŠ¨è·å–
    meta_dim = full_dataset.meta_features.shape[1]
    model = CausalFusionModel(meta_features_dim=meta_dim).to(Config.DEVICE)

    # 3. ä¼˜åŒ–å™¨å’ŒæŸå¤±
    optimizer = optim.AdamW(model.parameters(), lr=Config.LR)

    # é‡è¦ï¼šBCEWithLogitsLoss è‡ªå¸¦ Sigmoidï¼Œä¸” reduction='none' é…åˆæˆ‘ä»¬çš„åŠ æƒé€»è¾‘
    # ä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸Š pos_weight=torch.tensor([10.0]) æ¥åº”å¯¹æ•°æ®ä¸å¹³è¡¡
    criterion = nn.BCEWithLogitsLoss(reduction="none")

    # 4. è®­ç»ƒå¾ªç¯
    best_auc = 0.0
    for epoch in range(Config.EPOCHS):
        # è®­ç»ƒ
        train_loss = train_one_epoch(
            model, train_loader, criterion, optimizer, Config.DEVICE, epoch
        )

        # éªŒè¯
        val_loss, val_auc = validate(model, val_loader, criterion, Config.DEVICE)

        print(f"Epoch {epoch + 1}/{Config.EPOCHS}")
        print(
            f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}"
        )

        # ä¿å­˜æœ€å¥½çš„æ¨¡å‹
        if val_auc > best_auc:
            best_auc = val_auc
            torch.save(model.state_dict(), "./saved_models/best_model.pth")
            print("ğŸš€ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼")

    print("âœ… è®­ç»ƒç»“æŸï¼")


if __name__ == "__main__":
    main()
